<!DOCTYPE html>
<!--
  Algorithm Description:
  1. Initialization (onLoad):
     - Access the user's camera using `navigator.mediaDevices.getUserMedia`.
     - Initialize a 2D canvas to process video frames.
     - Create an `AR.Detector` for finding ArUco markers in the video stream.
     - Create a `POS.Posit` object to perform 3D pose estimation from the 2D marker corners. The POSIT algorithm requires the real-world size of the marker (modelSize) and the camera's focal length (approximated from canvas width).
     - Set up three.js renderers and scenes:
       - Two scenes (`scene1`, `scene2`) to visualize the two possible poses calculated by POSIT.
       - One main scene (`scene4`) to display the 3D model, overlaid on the video feed (`scene3`).
       - scene5 to display the 3D model with optim POSIT.

  2. Main Loop (tick):
     - On each animation frame, capture a frame from the video and draw it onto the hidden canvas (`snapshot`).
     - Process the canvas image data to find ArUco markers (`detector.detect`).
     - For each detected marker, draw its corners on the visible canvas (`drawCorners`).
     - If a marker is found, use its 2D corner coordinates to calculate the 3D pose (position and orientation) using the POSIT algorithm (`posit.pose`). POSIT returns two possible poses: a "best" one and an "alternative" one.
     - Update the three.js scenes (`updateScenes`):
       - The 3D model (`model`) is positioned and oriented using the `bestRotation` and `bestTranslation` from the POSIT result.
       - Two helper planes (`plane1`, `plane2`) are updated to show both the best and alternative poses separately.
       - The video stream is updated as a texture in the background of the main scene.
     - Display the numerical pose data (translation, rotation, error) for both poses on the screen.
     - Render all three.js scenes (`render`). The main view combines the video background and the 3D model.
-->
<html>

<head>
  <title>Augmented Reality</title>

  <!-- Include three.js library for 3D rendering -->
  <script type="text/javascript" src="libs/Three.js"></script> 

  <!-- Include js-aruco2 specific scripts -->
  <script type="text/javascript" src="../../src/svd.js"></script> <!-- Singular Value Decomposition, a dependency for POSIT -->
  <script type="text/javascript" src="../../src/posit1.js"></script> <!-- POSIT algorithm for 3D pose estimation -->
  <script type="text/javascript" src="../../src/cv.js"></script> <!-- Computer Vision utility functions -->
  <script type="text/javascript" src="../../src/aruco.js"></script> <!-- ArUco marker detector -->

  <script>
    // Global variables for video, canvas, and image processing
    var video, canvas, context, imageData, detector, posit;
    
    // three.js scene components
    var renderer1, renderer2, renderer3, renderer5;
    var scene1, scene2, scene3, scene4, scene5;
    var camera1, camera2, camera3, camera4, camera5;
    var plane1, plane2, model, model5, texture;
    var lastPose = null;
    var lastChoice = "Best"; // For stabilization with bias
    var lastSmoothPose = null; // For exponential smoothing
    var lastSmoothQuaternion = null; // For quaternion SLERP smoothing
    
    // A variable for animating the model
    var step = 0.0;

    // The real-world size of the marker in millimeters. This is crucial for correct pose estimation.
    var modelSize = 30.0; //millimeters

    // Function called when the window has finished loading
    function onLoad(){
      // Get HTML elements
      video = document.getElementById("video");
      canvas = document.getElementById("canvas");
      context = canvas.getContext("2d");
    
      // Set canvas dimensions based on its CSS style
      canvas.width = parseInt(canvas.style.width);
      canvas.height = parseInt(canvas.style.height);
      
      // Polyfill for navigator.mediaDevices.getUserMedia for cross-browser compatibility
      if (navigator.mediaDevices === undefined) {
        navigator.mediaDevices = {};
      }
      
      if (navigator.mediaDevices.getUserMedia === undefined) {
        navigator.mediaDevices.getUserMedia = function(constraints) {
          var getUserMedia = navigator.webkitGetUserMedia || navigator.mozGetUserMedia;
          
          if (!getUserMedia) {
            return Promise.reject(new Error('getUserMedia is not implemented in this browser'));
          }

          return new Promise(function(resolve, reject) {
            getUserMedia.call(navigator, constraints, resolve, reject);
          });
        }
      }
      
      // Request access to the user's camera
      navigator.mediaDevices
        .getUserMedia({ video: true })
        .then(function(stream) {
          // Attach the camera stream to the video element
          if ("srcObject" in video) {
            video.srcObject = stream;
          } else {
            video.src = window.URL.createObjectURL(stream);
          }
        })
        .catch(function(err) {
          // Log any errors to the console
          console.log(err.name + ": " + err.message);
        }
      );
      
      // Initialize the ArUco marker detector
      detector = new AR.Detector();
      // Initialize the POSIT object for pose estimation, passing the marker size and canvas width (as a proxy for camera focal length)
      posit = new POS.Posit(modelSize, canvas.width);

      // Set up the three.js renderers and scenes
      createRenderers();
      createScenes();

      // Start the main application loop
      requestAnimationFrame(tick);
    };

    // The main loop, called for each animation frame
    function tick(){
      // Request the next frame
      requestAnimationFrame(tick);
      
      // Check if the video has a frame ready to be processed
      if (video.readyState === video.HAVE_ENOUGH_DATA){
        // Capture a snapshot of the current video frame
        snapshot();

        // Detect markers in the snapshot
        var markers = detector.detect(imageData);
        // Draw the corners of the detected markers on the canvas
        drawCorners(markers);
        // Update the 3D scenes based on the detected markers
        updateScenes(markers);
        
        // Render the 3D scenes
        render();
      }
    };

    // Captures a frame from the video element and stores its pixel data
    function snapshot(){
      // Draw the current video frame onto the 2D canvas
      context.drawImage(video, 0, 0, canvas.width, canvas.height);
      // Get the image data from the canvas
      imageData = context.getImageData(0, 0, canvas.width, canvas.height);
    };
    
    // Draws the corners of detected markers on the canvas
    function drawCorners(markers){
      var corners, corner, i, j;
    
      context.lineWidth = 3;

      // Loop through all detected markers
      for (i = 0; i < markers.length; ++ i){
        corners = markers[i].corners;
        
        // Draw a red border around the marker
        context.strokeStyle = "red";
        context.beginPath();
        
        for (j = 0; j < corners.length; ++ j){
          corner = corners[j];
          context.moveTo(corner.x, corner.y);
          corner = corners[(j + 1) % corners.length];
          context.lineTo(corner.x, corner.y);
        }

        context.stroke();
        context.closePath();
        
        // Draw a small green square at the first corner to show orientation
        context.strokeStyle = "green";
        context.strokeRect(corners[0].x - 2, corners[0].y - 2, 4, 4);
      }
    };

    // Sets up the three.js WebGL renderers and scenes
    function createRenderers(){
      // Renderer for the first pose solution
      renderer1 = new THREE.WebGLRenderer();
      renderer1.setClearColor(0xffff00, 1);
      renderer1.setSize(canvas.width, canvas.height);
      document.getElementById("container1").appendChild(renderer1.domElement);
      scene1 = new THREE.Scene();
      camera1 = new THREE.PerspectiveCamera(40, canvas.width / canvas.height, 1, 1000);
      scene1.add(camera1);

      // Renderer for the second (alternative) pose solution
      renderer2 = new THREE.WebGLRenderer();
      renderer2.setClearColor(0xffff00, 1);
      renderer2.setSize(canvas.width, canvas.height);
      document.getElementById("container2").appendChild(renderer2.domElement);
      scene2 = new THREE.Scene();
      camera2 = new THREE.PerspectiveCamera(40, canvas.width / canvas.height, 1, 1000);
      scene2.add(camera2);

      // Main renderer for the combined AR view
      renderer3 = new THREE.WebGLRenderer();
      renderer3.setClearColor(0xffffff, 1);
      renderer3.setSize(canvas.width, canvas.height);
      document.getElementById("container").appendChild(renderer3.domElement);
      
      // Scene for the video background texture
      scene3 = new THREE.Scene();
      camera3 = new THREE.OrthographicCamera(-0.5, 0.5, 0.5, -0.5);
      scene3.add(camera3);
      
      // Scene for the 3D model
      scene4 = new THREE.Scene();
      camera4 = new THREE.PerspectiveCamera(40, canvas.width / canvas.height, 1, 1000);
      scene4.add(camera4);

      // Renderer for the 5th, stabilized view
      renderer5 = new THREE.WebGLRenderer();
      renderer5.setClearColor(0xffffff, 1);
      renderer5.setSize(canvas.width, canvas.height);
      document.getElementById("container5").appendChild(renderer5.domElement);
      scene5 = new THREE.Scene();
      camera5 = new THREE.PerspectiveCamera(40, canvas.width / canvas.height, 1, 1000);
      scene5.add(camera5);
    };

    // Renders all the scenes
    function render(){
      // Render scene1 with the first pose
      renderer1.clear();
      renderer1.render(scene1, camera1);
      
      // Render scene2 with the alternative pose
      renderer2.clear();
      renderer2.render(scene2, camera2);

      // To overlay the 3D model on the video, we render two scenes with the same renderer
      // without clearing the buffer in between.
      renderer3.autoClear = false;
      renderer3.clear();
      // Render the video background first
      renderer3.render(scene3, camera3);
      // Render the 3D model on top
      renderer3.render(scene4, camera4);

      // Render the 5th scene (stabilized)
      renderer5.autoClear = false;
      renderer5.clear();
      renderer5.render(scene3, camera3); // Re-use the video texture scene
      renderer5.render(scene5, camera5);
    };

    // Creates the 3D objects for the scenes
    function createScenes(){
      // Create a plane to visualize the first pose
      plane1 = createPlane();
      scene1.add(plane1);

      // Create a plane to visualize the second pose
      plane2 = createPlane();
      scene2.add(plane2);

      // Create a plane with the video as a texture for the background
      texture = createTexture();
      scene3.add(texture);

      // Create the 3D model to be augmented
      model = createModel();
      scene4.add(model);

      // Create the second 3D model for the stabilized view
      model5 = createModel();
      scene5.add(model5);
    };
    
    // Helper function to create a simple plane object
    function createPlane(){
      var object = new THREE.Object3D(),
          geometry = new THREE.PlaneGeometry(1.0, 1.0, 0.0),
          material = new THREE.MeshNormalMaterial(), // Material that maps normals to RGB colors
          mesh = new THREE.Mesh(geometry, material);
      
      // Set the rotation order. Important for avoiding gimbal lock.
      object.eulerOrder = 'YXZ';
      
      object.add(mesh);
      
      return object;
    };
    
    // Creates a plane with the video feed as its texture
    function createTexture(){
      // Use the video element as a texture
      var texture = new THREE.Texture(video),
          object = new THREE.Object3D(),
          geometry = new THREE.PlaneGeometry(1.0, 1.0, 0.0),
          // Use a basic material with the video texture. Disable depth testing to ensure it's always in the background.
          material = new THREE.MeshBasicMaterial( {map: texture, depthTest: false, depthWrite: false} ),
          mesh = new THREE.Mesh(geometry, material);

      // Fix texture warning: set filters for non-power-of-two textures
      texture.minFilter = THREE.LinearFilter;
      texture.magFilter = THREE.LinearFilter;
      texture.generateMipmaps = false;

      object.position.z = -1;

      object.add(mesh);

      return object;
    };
    
    // Creates the 3D sphere model
    function createModel(){
      var object = new THREE.Object3D(),
          geometry = new THREE.CylinderGeometry(0, 0.5, 1.0, 16),
          texture = THREE.ImageUtils.loadTexture("texture_green.jpg"),
          material = new THREE.MeshBasicMaterial( {map: texture} ),
          mesh = new THREE.Mesh(geometry, material);
      
      object.add(mesh);
      
      return object;
    };

    function updateScenes(markers){
      var corners, corner, pose, i;
      
      if (markers.length > 0){
        // Use the corners of the first detected marker
        corners = markers[0].corners;
        
        // The POSIT algorithm requires points in a coordinate system where the center of the
        // screen is the origin. Convert the corner coordinates from canvas coordinates.
        for (i = 0; i < corners.length; ++ i){
          corner = corners[i];
          
          corner.x = corner.x - (canvas.width / 2);
          corner.y = (canvas.height / 2) - corner.y;
        }
        
        // Get the pose estimation from the POSIT algorithm
        pose = posit.pose(corners);
        
        // Update the original, unstable objects
        updateObject(plane1, pose.bestRotation, pose.bestTranslation);
        updateObject(plane2, pose.alternativeRotation, pose.alternativeTranslation);
        updateObject(model, pose.bestRotation, pose.bestTranslation);

        // --- Improved Stabilization Logic ---
        var stablePose;
        var currentChoice = "Best";

        if (lastPose === null) {
          // First frame or marker re-appeared: choose based on error metric
          if (pose.bestError <= pose.alternativeError) {
            stablePose = { rotation: pose.bestRotation, translation: pose.bestTranslation };
            currentChoice = "Best";
          } else {
            stablePose = { rotation: pose.alternativeRotation, translation: pose.alternativeTranslation };
            currentChoice = "Alternative";
          }
        } else {
          // Compare both rotation AND translation to decide which pose to use
          var bestScore = comparePoses(lastPose, pose.bestRotation, pose.bestTranslation);
          var altScore = comparePoses(lastPose, pose.alternativeRotation, pose.alternativeTranslation);

          // Add hysteresis: prefer last choice unless the other is significantly better
          var hysteresis = 0.15; // 15% hysteresis threshold

          if (lastChoice === "Best") {
            currentChoice = (altScore < bestScore * (1 - hysteresis)) ? "Alternative" : "Best";
          } else {
            currentChoice = (bestScore < altScore * (1 - hysteresis)) ? "Best" : "Alternative";
          }

          stablePose = (currentChoice === "Best")
            ? { rotation: pose.bestRotation, translation: pose.bestTranslation }
            : { rotation: pose.alternativeRotation, translation: pose.alternativeTranslation };
        }

        // Apply exponential smoothing to the final pose using quaternion SLERP
        if (lastSmoothQuaternion !== null && lastSmoothQuaternion !== undefined && 
            lastSmoothPose !== null && lastSmoothPose !== undefined) {
          // Convert current rotation matrix to quaternion
          var currentQ = rotationToQuaternion(stablePose.rotation);
          
          // Check if the angle change is too large (more than 30 degrees)
          var dot = Math.abs(lastSmoothQuaternion[0]*currentQ[0] + lastSmoothQuaternion[1]*currentQ[1] + 
                            lastSmoothQuaternion[2]*currentQ[2] + lastSmoothQuaternion[3]*currentQ[3]);
          var angleChange = Math.acos(Math.min(dot, 1.0)) * 2 * 180 / Math.PI; // in degrees
          
          if (angleChange > 30) {
            // Reject large rotation changes - use previous rotation
            stablePose.rotation = quaternionToRotation(lastSmoothQuaternion);
            console.log("Large rotation rejected (change: " + angleChange.toFixed(1) + "°)");
          } else {
            // SLERP between quaternions (0.15 = stronger smoothing)
            var smoothedQ = slerp(lastSmoothQuaternion, currentQ, 0.15);
            
            // Convert back to rotation matrix
            stablePose.rotation = quaternionToRotation(smoothedQ);
          }
          
          // Also smooth translation linearly with limit
          var transChange = Math.sqrt(
            Math.pow(stablePose.translation[0] - lastSmoothPose.translation[0], 2) +
            Math.pow(stablePose.translation[1] - lastSmoothPose.translation[1], 2) +
            Math.pow(stablePose.translation[2] - lastSmoothPose.translation[2], 2)
          );
          
          if (transChange > 20) {
            // Limit translation change to 20mm per frame
            var scale = 20 / transChange;
            for (var i = 0; i < 3; i++) {
              stablePose.translation[i] = lastSmoothPose.translation[i] + 
                (stablePose.translation[i] - lastSmoothPose.translation[i]) * scale * 0.15;
            }
          } else {
            for (var i = 0; i < 3; i++) {
              stablePose.translation[i] = lastSmoothPose.translation[i] * 0.85 + stablePose.translation[i] * 0.15;
            }
          }
          
          console.log("Smoothing applied");
        }

        // Store state for next frame
        lastPose = stablePose;
        lastChoice = currentChoice;
        lastSmoothPose = stablePose;
        // Store smoothed quaternion for next frame
        var currentQ = rotationToQuaternion(stablePose.rotation);
        if (lastSmoothQuaternion === null || lastSmoothQuaternion === undefined) {
          lastSmoothQuaternion = currentQ;
        } else {
          // Check angle change for storage too
          var dot = Math.abs(lastSmoothQuaternion[0]*currentQ[0] + lastSmoothQuaternion[1]*currentQ[1] + 
                            lastSmoothQuaternion[2]*currentQ[2] + lastSmoothQuaternion[3]*currentQ[3]);
          var angleChange = Math.acos(Math.min(dot, 1.0)) * 2 * 180 / Math.PI;
          
          if (angleChange > 30) {
            // Don't update quaternion if change is too large
            console.log("Quaternion update rejected (change: " + angleChange.toFixed(1) + "°)");
          } else {
            lastSmoothQuaternion = slerp(lastSmoothQuaternion, currentQ, 0.15);
          }
        }

        // Log diagnostic data
        console.log("--- Debug Log ---");
        console.log("Best Pose Error: " + pose.bestError.toFixed(4));
        console.log("Alt. Pose Error: " + pose.alternativeError.toFixed(4));
        console.log("Chosen Pose: " + currentChoice);
        console.log("Translation: [" + stablePose.translation[0].toFixed(1) + ", " + stablePose.translation[1].toFixed(1) + ", " + stablePose.translation[2].toFixed(1) + "]");
        
        // Log rotation angles for debugging
        var yaw = -Math.atan2(stablePose.rotation[0][2], stablePose.rotation[2][2]);
        var pitch = -Math.asin(-stablePose.rotation[1][2]);
        var roll = Math.atan2(stablePose.rotation[1][0], stablePose.rotation[1][1]);
        console.log("Rotation [yaw, pitch, roll]: [" + (yaw * 180/Math.PI).toFixed(1) + ", " + (pitch * 180/Math.PI).toFixed(1) + ", " + (roll * 180/Math.PI).toFixed(1) + "]");


        // Update the stabilized model
        updateObject(model5, stablePose.rotation, stablePose.translation);

        // Display the numerical data for each pose
        updatePose("pose1", pose.bestError, pose.bestRotation, pose.bestTranslation);
        updatePose("pose2", pose.alternativeError, pose.alternativeRotation, pose.alternativeTranslation);
        
        // Add a simple animation to the models (only for non-stabilized model)
        step += 0.025;
        model.rotation.y -= step;
        // model5.rotation.y -= step; // Disabled for stabilized model to show pure POSIT result

      } else {
        // Reset when marker is lost - but keep lastSmoothQuaternion for when marker re-appears
        lastPose = null;
        lastSmoothPose = null;
        // Don't reset lastSmoothQuaternion - keep it for smooth transition when marker re-appears
        console.log("--- Debug Log ---\nNo markers detected.");
      }
      
      // This is crucial! It tells three.js to update the video texture on the next render.
      texture.children[0].material.map.needsUpdate = true;
    };
    
    // Helper function to calculate the dot product of two 3D vectors
    function dot(v1, v2) {
      return v1[0] * v2[0] + v1[1] * v2[1] + v1[2] * v2[2];
    }

    // Convert rotation matrix to quaternion
    function rotationToQuaternion(rot) {
      var m00 = rot[0][0], m01 = rot[0][1], m02 = rot[0][2];
      var m10 = rot[1][0], m11 = rot[1][1], m12 = rot[1][2];
      var m20 = rot[2][0], m21 = rot[2][1], m22 = rot[2][2];
      
      var trace = m00 + m11 + m22;
      var qw, qx, qy, qz;

      if (trace > 0) {
        var s = 0.5 / Math.sqrt(trace + 1.0);
        qw = 0.25 / s;
        qx = (m21 - m12) * s;
        qy = (m02 - m20) * s;
        qz = (m10 - m01) * s;
      } else if (m00 > m11 && m00 > m22) {
        var s = 2.0 * Math.sqrt(1.0 + m00 - m11 - m22);
        qw = (m21 - m12) / s;
        qx = 0.25 * s;
        qy = (m01 + m10) / s;
        qz = (m02 + m20) / s;
      } else if (m11 > m22) {
        var s = 2.0 * Math.sqrt(1.0 + m11 - m00 - m22);
        qw = (m02 - m20) / s;
        qx = (m01 + m10) / s;
        qy = 0.25 * s;
        qz = (m12 + m21) / s;
      } else {
        var s = 2.0 * Math.sqrt(1.0 + m22 - m00 - m11);
        qw = (m10 - m01) / s;
        qx = (m02 + m20) / s;
        qy = (m12 + m21) / s;
        qz = 0.25 * s;
      }

      // Normalize quaternion
      var len = Math.sqrt(qw*qw + qx*qx + qy*qy + qz*qz);
      return [qw/len, qx/len, qy/len, qz/len];
    }

    // Convert quaternion to rotation matrix
    function quaternionToRotation(q) {
      var qw = q[0], qx = q[1], qy = q[2], qz = q[3];
      var rot = [[0,0,0],[0,0,0],[0,0,0]];
      
      var xx = qx*qx, yy = qy*qy, zz = qz*qz;
      var xy = qx*qy, xz = qx*qz, yz = qy*qz;
      var xw = qx*qw, yw = qy*qw, zw = qz*qw;

      rot[0][0] = 1 - 2*(yy + zz);
      rot[0][1] = 2*(xy - zw);
      rot[0][2] = 2*(xz + yw);
      rot[1][0] = 2*(xy + zw);
      rot[1][1] = 1 - 2*(xx + zz);
      rot[1][2] = 2*(yz - xw);
      rot[2][0] = 2*(xz - yw);
      rot[2][1] = 2*(yz + xw);
      rot[2][2] = 1 - 2*(xx + yy);

      return rot;
    }

    // Spherical Linear Interpolation (SLERP) for quaternions
    function slerp(q1, q2, t) {
      // Dot product
      var dot = q1[0]*q2[0] + q1[1]*q2[1] + q1[2]*q2[2] + q1[3]*q2[3];
      
      // If dot product is negative, negate one quaternion to take shortest path
      if (dot < 0) {
        q2 = [-q2[0], -q2[1], -q2[2], -q2[3]];
        dot = -dot;
      }
      
      // If quaternions are very close, use linear interpolation
      if (dot > 0.9995) {
        var result = [
          q1[0] + t*(q2[0] - q1[0]),
          q1[1] + t*(q2[1] - q1[1]),
          q1[2] + t*(q2[2] - q1[2]),
          q1[3] + t*(q2[3] - q1[3])
        ];
        var len = Math.sqrt(result[0]*result[0] + result[1]*result[1] + result[2]*result[2] + result[3]*result[3]);
        return [result[0]/len, result[1]/len, result[2]/len, result[3]/len];
      }
      
      var theta0 = Math.acos(dot);
      var theta = theta0 * t;
      var sinTheta = Math.sin(theta0);
      var sinThetaNew = Math.sin(theta);
      
      var s1 = Math.cos(theta) - dot * sinThetaNew / sinTheta;
      var s2 = sinThetaNew / sinTheta;
      
      return [
        s1*q1[0] + s2*q2[0],
        s1*q1[1] + s2*q2[1],
        s1*q1[2] + s2*q2[2],
        s1*q1[3] + s2*q2[3]
      ];
    }

    // Compare two poses by computing combined rotation + translation distance
    function comparePoses(lastPose, rotation, translation) {
      // Rotation difference: sum of squared differences of rotation matrix elements
      var rotDiff = 0;
      for (var i = 0; i < 3; i++) {
        for (var j = 0; j < 3; j++) {
          var diff = lastPose.rotation[i][j] - rotation[i][j];
          rotDiff += diff * diff;
        }
      }

      // Translation difference
      var transDiff = 0;
      for (var i = 0; i < 3; i++) {
        var diff = lastPose.translation[i] - translation[i];
        transDiff += diff * diff;
      }

      // Normalize translation difference (approximate marker distance ~100mm)
      var normalizedTransDiff = transDiff / 10000;

      // Combined score (lower is better)
      return rotDiff + normalizedTransDiff;
    }

    // Applies the calculated rotation and translation to a three.js object
    function updateObject(object, rotation, translation){
      // Scale the object to the size of the marker
      object.scale.x = modelSize;
      object.scale.y = modelSize;
      object.scale.z = modelSize;

      // Convert rotation matrix to quaternion using helper function
      var q = rotationToQuaternion(rotation);
      object.quaternion.set(q[0], q[1], q[2], q[3]);

      // Apply the translation. Note that the Z axis is inverted.
      object.position.x = translation[0];
      object.position.y = translation[1];
      object.position.z = -translation[2];
      
      // Force matrix update for old Three.js version (r70)
      object.updateMatrix();
      object.matrixWorldNeedsUpdate = true;
    };
    
    // Displays the pose information as text on the webpage
    function updatePose(id, error, rotation, translation){
      // Convert rotation matrix to yaw, pitch, and roll
      var yaw = -Math.atan2(rotation[0][2], rotation[2][2]);
      var pitch = -Math.asin(-rotation[1][2]);
      var roll = Math.atan2(rotation[1][0], rotation[1][1]);
      
      // Update the innerHTML of the specified element with the pose data
      var d = document.getElementById(id);
      d.innerHTML = " error: " + error
                  + "<br/>"
                  + " x: " + (translation[0] | 0) // Bitwise OR 0 to truncate to integer
                  + " y: " + (translation[1] | 0)
                  + " z: " + (translation[2] | 0)
                  + "<br/>"
                  // Convert radians to degrees for display
                  + " yaw: " + Math.round(-yaw * 180.0/Math.PI)
                  + " pitch: " + Math.round(-pitch * 180.0/Math.PI)
                  + " roll: " + Math.round(roll * 180.0/Math.PI);
    };

    // Set the onLoad function to be executed when the window loads
    window.onload = onLoad;
  </script>

</head>

<body style="text-align: center; font-family: monospace;">

  <!-- The video element will stream the camera feed, but it's not displayed directly. -->
  <video id="video" width=320 height=240 autoplay="true" style="display:none;"></video>
  
  <div style="margin: 10px;"><strong>-= Augmented Reality =-</strong></div>
  <div style="width: 100%;">
    <div style="width: 670px; margin-left:auto; margin-right:auto;">

      <!-- Row 1 -->
      <div style="float: left; margin-right: 10px;">
        <div>1. Video Feed with Detected Markers</div>
        <canvas id="canvas" style="width: 320px; height: 240px; border: solid 1px black;"></canvas>
      </div>
      
      <div style="float: left;">
        <div>2. Main AR View (Video + 3D Model)</div>
        <div id="container" style="width: 320px; height: 240px; border: solid 1px black; background: green;"></div>
      </div>

      <!-- Row 2 -->
      <div style="float: left; margin-right: 10px;">
        <div style="border: solid 1px black; padding: 2px;">
          <div>3. Best Pose Visualization</div>
          <div id="container1" style="width: 320px; height: 240px; background: red;"></div>
          <div id="pose1"></div>
        </div>
      </div>
      
      <div style="float: left;">
        <div style="border: solid 1px black; padding: 2px;">
          <div>4. Alternative Pose Visualization</div>
          <div id="container2" style="width: 320px; height: 240px; background: blue;"></div>
          <div id="pose2"></div>
        </div>
      </div>

      <div style="clear: both; margin-bottom: 10px;"></div>

      <!-- Centered 5th frame -->
      <div style="float: left; width: 100%; text-align: center; margin-top: 10px;">
        <div style="border: solid 1px black; padding: 2px; display: inline-block;">
            <div>5. Stabilized AR View</div>
            <div id="container5" style="width: 320px; height: 240px; background: purple;"></div>
        </div>
      </div>
    </div>
  <div style="margin: 15px;"><strong>Powered by <a href="https://damianofalcioni.github.io/js-aruco2/">js-aruco2</a> and <a href="https://github.com/mrdoob/three.js">Three.js</a></strong></div>

</body>
  
</html>